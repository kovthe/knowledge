{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython.display as disp\n",
    "import scipy.stats as st\n",
    "import math\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science<br/> Lesson 3-4: Data Analysis and Visualization\n",
    "The following notes are a summary on [Udacity's online course](https://www.udacity.com/course/intro-to-data-science--ud359)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Ideas\n",
    "* Welch's Two Sample t-Test\n",
    "* Shapiro-Wilk Test\n",
    "* Non-parametric tests\n",
    "* Mann-Whitney U test\n",
    "* Machine Learning vs Statistics\n",
    "* Linear Regression - Gradient Descent\n",
    "* Linear Regression - OLS\n",
    "* Coefficient of Determination\n",
    "\n",
    "## Statistical Rigor\n",
    "When performing an analysis, we can usually achieve statistical rigor by way of a significance test. These tests find whether or not a sample of data can disprove some conventional wisdom or assumption, with a predefined level of confidence. Using these tests we can assess the feasibility of a result that we get with a smaller sample when trying to say something about a larger population. \n",
    "\n",
    "You can also imagine a case where at first glance, our results might suggest that there's no significant difference but it turns out that there is.\n",
    "\n",
    "Statistical tests are useful to evaluate whether perceived effects in our dataset reflects across the whole population.\n",
    "\n",
    "## Statistical Significance Tests\n",
    "### t-test\n",
    "* Accept or reject a null hypthesis\n",
    "* Null Hypothesis: A statement we are trying to disprove by running our test.\n",
    "\n",
    "### Welch's Two Sample t-Test\n",
    "The Welch's Two sample t-Test does not assume there is equal sample size or the same variance.\n",
    "\n",
    "$$ t = \\frac{\\mu_1 - \\mu_2}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_2^2}{N_2}}}$$\n",
    "\n",
    "$\\mu_i = $ sample mean for $i^{th}$ sample  \n",
    "$\\sigma_i = $ sample variance for $i^{th}$ sample  \n",
    "$N_i = $ sample size for $i^{th}$ sample\n",
    "\n",
    "The degrees of freedom $\\nu$ associated is calculated with:\n",
    "\n",
    "$$\\nu \\quad  \\approx \\quad \n",
    " {{\\left( \\; {\\sigma_1^2 \\over N_1} \\; + \\; {\\sigma_2^2 \\over N_2} \\; \\right)^2 } \\over\n",
    " { \\quad {\\sigma_1^4 \\over N_1^2 \\nu_1} \\; + \\; {\\sigma_2^4 \\over N_2^2 \\nu_2 } \\quad }}$$\n",
    "\n",
    "$v_i = N_i - 1$ \n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_averages(filename):\n",
    "    \"\"\"\n",
    "    Performs a t-test on two sets of baseball data (left-handed and right-handed hitters).\n",
    "\n",
    "    You will be given a csv file that has three columns.  A player's\n",
    "    name, handedness (L for lefthanded or R for righthanded) and their\n",
    "    career batting average (called 'avg'). You can look at the csv\n",
    "    file by downloading the baseball_stats file from Downloadables below. \n",
    "    \n",
    "    Write a function that will read that the csv file into a pandas data frame,\n",
    "    and run Welch's t-test on the two cohorts defined by handedness.\n",
    "    \n",
    "    One cohort should be a data frame of right-handed batters. And the other\n",
    "    cohort should be a data frame of left-handed batters.\n",
    "    \n",
    "    We have included the scipy.stats library to help you write\n",
    "    or implement Welch's t-test:\n",
    "    http://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "    \n",
    "    With a significance level of 95%, if there is no difference\n",
    "    between the two cohorts, return a tuple consisting of\n",
    "    True, and then the tuple returned by scipy.stats.ttest.  \n",
    "    \n",
    "    If there is a difference, return a tuple consisting of\n",
    "    False, and then the tuple returned by scipy.stats.ttest.\n",
    "    \n",
    "    For example, the tuple that you return may look like:\n",
    "    (True, (9.93570222, 0.000023))\n",
    "    \"\"\"\n",
    "    df = pandas.read_csv(filename)\n",
    "    avgLeft  = df[df[\"handedness\"] == \"L\"].avg\n",
    "    avgRight = df[df[\"handedness\"] == \"R\"].avg\n",
    "    \n",
    "    # Perform Welch's Test\n",
    "    # t_result returns (t-value, p-value)\n",
    "    t_result = scipy.stats.ttest_ind(avgLeft, avgRight, equal_var=False)\n",
    "    \n",
    "    if t_result[1] <= .05:\n",
    "        return (False, result)\n",
    "    else:\n",
    "        return(True, result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Normal Data\n",
    "When performing the t-Test, we assume the data is normal. In the wild, often times, the distributions that are encountered are not normal. There are some statistical tests that we can use to measure the likelihood that a distribution is likely normal. One such test is the **shapiro-wilk test**:\n",
    "\n",
    "```python\n",
    "# data is an array or list containing all the data points\n",
    "w,p = st.shapiro(data)\n",
    "```\n",
    "w is the test statistic, p is the p-value.\n",
    "\n",
    "### What if our data is non-normal?\n",
    "If we have enough data, we can still use tests that assume the data is normal like the t-Test. We can also use a **non-parametric test**, which is a statistical test that does not assume our data is drawn from any particular underlying probability distribution.\n",
    "\n",
    "**Mann-Whitney U test** - tests null hypothesis that two samples come from the same population.\n",
    "\n",
    "```python\n",
    "u,p = scipy.stats.mannwhitneyu(x,y)\n",
    "```\n",
    "u = test statistic, p = one sided p-value\n",
    "\n",
    "Does not test which sample has a higher mean or a higher median, etc. So it's usually useful to present the results of this test with the samples means, medians, etc.\n",
    "\n",
    "## Machine Learning\n",
    "Machine learning is a branch of AI focused on constructing systems that make predictions by learning from large amounts of data.\n",
    "\n",
    "### Differences between Machine Learning and Statistics\n",
    "* Statistics is focused on analyzing existing data and drawing valid conclusions.\n",
    "* Machine learning is focused on making predictions.\n",
    "\n",
    "### Types of Machine Learning\n",
    "* Supervised Learning\n",
    "  * Teaching our model what the correct answer looks like. Providing trading examples with characteristics and the correct answer, in other words know the input and output.\n",
    "* Unsupervised Learning\n",
    "  * Have unlabeled data points so we are trying to understand the data.\n",
    "  * Finding out what types of characteristics cluster with each other.\n",
    "\n",
    "### Gradient Descent\n",
    "Gradient descent is an example of supervised learning. Linear regression with gradient descent can be used to determine coefficients that minimize the cost function.\n",
    "\n",
    "An alternative to gradient descent to find the minimum J, the cost function, is the analytical ordinary least squares approach. However as the number of features that are used increases, ordinary least squares can be quite computationally intensive. Gradient descent can give a much faster solution.\n",
    "\n",
    "## Linear regression in Python\n",
    "There are many Python libraries implementing linear regression. For the following exercise, you should use either OLS from StatsModels (documentation [here](http://statsmodels.sourceforge.net/0.5.0/generated/statsmodels.regression.linear_model.OLS.html) or LinearRegression from Scikit Learn, both of which implement linear regression using ordinary least squares. If you have not used either before, we recommend starting with StatsModels. If you are interested in using a gradient descent implementation, take a look at the last exercise in Problem Set 3.\n",
    "\n",
    "### Exercise instructions\n",
    "\n",
    "In the following exercise, you will calculate the parameters or weights, that is, the values of theta, to predict how many home runs a baseball player will hit given their height and weight\n",
    "\n",
    "### Advice specific to StatsModels\n",
    "The linked documentation for StatsModels shows the input array, X, being one-dimensional, which corresponds to only having one feature. It will also support a two-dimensional array with m rows and n columns, where `m` is the number of data points and `n` is the number of features for each data point.\n",
    "\n",
    "Most of the time, when you are performing linear regression, you will want to include an intercept. That is, if you are predicting y as a function of a two input variables x1 and x2, you want to produce an equation of the form `y = m1*x1 + m2*x2 + b` rather than just `y = m1*x1 + m2*x2`. In this equation, `b` is called the \"intercept\". StatsModels does not use an intercept, but you can add an intercept by adding another \"feature\" which has value 1 for every data point. (Thus the equation becomes y = b*1 + m1*x1 + m2*x2.) This is what the call to add_constant does in the example code, and you should include a similar call in your code if you use the StatsModels library. Note that add_constant adds the constant as the first feature rather than the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ordinary Least Squares Example\n",
    "\n",
    "def linear_regression(features, values):\n",
    "    \"\"\"\n",
    "    Performs linear regression given a dataset with an arbitrary number of features.\n",
    "    'features' is the input data points (or the X's) and 'values' is the output data points\n",
    "    (or the Y's).\n",
    "    \n",
    "    Returns the intercept and the parameters, that is, the optimal values of theta.\n",
    "    \n",
    "    This page contains example code that may be helpful:\n",
    "    http://statsmodels.sourceforge.net/0.5.0/generated/statsmodels.regression.linear_model.OLS.html\n",
    "    \"\"\"\n",
    "\n",
    "    features = sm.add_constant(features)\n",
    "    model = sm.OLS(values, features)\n",
    "    results = model.fit()\n",
    "    intercept = results.params[0]\n",
    "    params = results.params[1:]\n",
    "    return intercept, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `add_constant` adds the constant feature as the first feature, so the intercept is at index 0 of the array `results.params`. The remaining values of `results.params`, from index 1 to the end, are the parameters, or weights, of the real features.\n",
    "\n",
    "Also note that `values` comes before `features` when creating the OLS model, just as `Y` came before `X` in the example code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination\n",
    "One way of measuring the effectiveness of our linear regression model:\n",
    "For:  \n",
    "data = $y_i$....$y_n$  \n",
    "predictions = $f_i$....$f_n$  \n",
    "average of data = $\\bar{y}$\n",
    "$$R^2 = 1 - \\frac{\\sum{(y_i - f_i)^2}}{\\sum{(y_i-\\bar{y})^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_r_squared(data, predictions):\n",
    "    # Write a function that, given two input numpy arrays, 'data', and 'predictions,'\n",
    "    # returns the coefficient of determination, R^2, for the model that produced \n",
    "    # predictions.\n",
    "    # \n",
    "    # Numpy has a couple of functions -- np.mean() and np.sum() --\n",
    "    # that you might find useful, but you don't have to use them.\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    SST = ((data-np.mean(data))**2).sum()\n",
    "    SSReg = ((predictions-data)**2).sum()\n",
    "    r_squared = 1 - SSReg / SST\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Considerations\n",
    "* Parameter estimation\n",
    "  * Confidence Intervals on thetas (What is the likelihood we would calculate this parameter value if the parameter had no effect)\n",
    "  * over/underfitting\n",
    "    * Use cross validation (training set) and learning set.\n",
    "    * Need to make sure we're not trying to fit a linear line on nonlinear data.\n",
    "  * Multiple local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    div.CodeMirror code{ /* code font */\n",
       "        font-family: \"Consolas\", monospace;\n",
       "        font-size: 10pt;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        font-size: 115%;\n",
       "        padding: 0;\n",
       "    }\n",
       "    /* header colours and fonts */\n",
       "    h1 {\n",
       "        color: #444;\n",
       "    }\n",
       "    h2 { color: #444; }\n",
       "    h3\n",
       "    {\n",
       "         color: #444;\n",
       "         font-style: italic;\n",
       "         font-weight: normal;\n",
       "         font-size: 110% !important;\n",
       "         margin-top: 0.6em !important;\n",
       "    }\n",
       "    h4\n",
       "    {\n",
       "        margin-top: 0.5em !important;\n",
       "        color: #444;\n",
       "    }\n",
       "    h5 { color: #444; }\n",
       "    h6 { color: #444; }\n",
       "\n",
       "    ul {margin-top: 0em !important}\n",
       "    p {margin-top: 0.4em !important}\n",
       "\n",
       "    div.output_subarea\n",
       "    {\n",
       "      padding: 1em;\n",
       "    }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "/*              displayIndent: \"4em\",*/\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def css_styling():\n",
    "    styles = open(\"../css/custom.css\", \"r\").read()\n",
    "    return disp.HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
